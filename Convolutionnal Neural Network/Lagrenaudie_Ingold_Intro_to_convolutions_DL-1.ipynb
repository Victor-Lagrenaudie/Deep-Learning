{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of convolutions with tensorflow\n",
    "\n",
    "In this notebook, you'll be using tensorflow to build a Convolutional Neural Network (CNN).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "Both, [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/Convolution.ipynb) and this [wikipedia page](https://en.wikipedia.org/wiki/Convolution) might help you understand what is a convolution.\n",
    "\n",
    "no, if we consider two functions $f$ and $g$ taking values from $\\mathbb{Z} \\to \\mathbb{R}$ then:  \n",
    "$ (f * g)[n] = \\sum_{m = -\\infty}^{+\\infty} f[m] \\cdot g[n - m] $\n",
    "\n",
    "In our case, we consider the two vectors $x$ and $w$ :  \n",
    "$ x = (x_1, x_2, ..., x_{n-1}, x_n) $  \n",
    "$ w = (w_1, w_2) $\n",
    "\n",
    "And get :   \n",
    "$ x * w = (w_1 x_1 + w_2 x_2, w_1 x_2 + w_2 x_3, ..., w_1 x_{n-1} + w_2 x_n)$\n",
    "\n",
    "\n",
    "#### Deep learning subtility :\n",
    "    \n",
    "In most of deep learning framewoks, you'll get to chose in between three paddings:\n",
    "- **Same**: $(f*g)$ has the same shape as x (we pad the entry with zeros)\n",
    "- **valid**: $(f*g)$ has the shape of x minus the shape of w plus 1 (no padding on x)\n",
    "- **Causal**: $(f*g)(n_t)$ does not depend on any $(n_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow\n",
    "\n",
    "\"TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and also used for machine learning applications such as neural networks.[3] It is used for both research and production at Google often replacing its closed-source predecessor, DistBelief.\" - Wikipedia\n",
    "\n",
    "We'll be using tensorflow to build the models we want to use. \n",
    "\n",
    "Here below, we build a AND gate with a very simple neural network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00839782]\n",
      " [ 0.1499088 ]\n",
      " [ 0.1499088 ]\n",
      " [ 0.78595555]] Tensor(\"Y:0\", shape=(?, 1), dtype=float32) b'\\n\\x10\\n\\tloss/loss\\x15\\xfa\\xf8\\x12>'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our Dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([0,0,0,1])\n",
    "Y=Y.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "# Define the tensorflow tensors\n",
    "x = tf.placeholder(tf.float32, [None, 2], name='X')  # inputs\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='Y')  # outputs\n",
    "W = tf.Variable(tf.zeros([2, 1]), name='W')\n",
    "b = tf.Variable(tf.zeros([1,]), name='b')\n",
    "\n",
    "\n",
    "# Define the model\n",
    "pred = tf.nn.sigmoid(tf.matmul(x, W) + b)  # Model\n",
    "\n",
    "# Define the loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred) + (1-y) * tf.log(1-pred), reduction_indices=1))\n",
    "    loss_summary = tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Define the optimizer method you want to use\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# Include some Tensorboard visualization\n",
    "writer = tf.summary.FileWriter(\"./my_model/\")\n",
    "\n",
    "\n",
    "# Start training session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        _, c, p, ls = sess.run([optimizer, loss, pred, loss_summary], feed_dict={x: X,\n",
    "                                                      y: Y})\n",
    "        writer.add_summary(ls,epoch)\n",
    "print(p, y, ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the graph you just created, launch tensorbord.  \n",
    "`$tensorboard --logdirs=./` on linux (with corresponding logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Get inspiration from the preceding code to build a XOR gate\n",
    "\n",
    "Design a neural network with 2 layers.\n",
    "- layer1 has 2 neurons (sigmoid or tanh activation)\n",
    "- Layer2 has 1 neuron (it outouts the prediction)\n",
    "\n",
    "And train  it\n",
    "\n",
    "It's **mandatory** that you get a **tensorboard visualization** of your graph, try to make it look good, plz :)\n",
    "\n",
    "Here below I put a graph of the model you want to have (yet your weights won't be the same)\n",
    "![graph](https://i.stack.imgur.com/nRZ6z.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.48681248e-06]\n",
      " [  9.99995232e-01]\n",
      " [  9.99995351e-01]\n",
      " [  3.73645389e-06]] Tensor(\"Y:0\", shape=(?, 1), dtype=float32)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our Dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([0,1,1,0]).reshape(-1,1)\n",
    "\n",
    "input_node=2\n",
    "hidden_node=2\n",
    "output_node=1\n",
    "\n",
    "# Define the tensorflow tensors\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name='X')# inputs\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='Y')# outputs\n",
    "#W1 = tf.Variable(tf.random_normal([2, 2], seed=0), name='W1')\n",
    "#W2 = tf.Variable(tf.random_normal([2, 1], seed=0), name='W2')\n",
    "#b1 = tf.Variable(tf.random_normal([2,], seed=0), name='b1')\n",
    "#b2= tf.Variable(tf.random_normal([1,], seed=0), name='b2')\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([input_node, hidden_node],seed=0), name='W1')\n",
    "W2 = tf.Variable(tf.random_normal([hidden_node, output_node],seed=0), name='W2')\n",
    "b1 = tf.Variable(tf.random_normal([hidden_node,],seed=0), name='b1')\n",
    "b2= tf.Variable(tf.random_normal([output_node,],seed=0), name='b2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "first_pred = tf.nn.sigmoid(tf.matmul(x, W1) + b1)# Model\n",
    "pred = tf.nn.sigmoid(tf.matmul(first_pred, W2) + b2)# Model\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred) + (1-y) * tf.log(1-pred), reduction_indices=1))\n",
    "    loss_summary=tf.summary.scalar('loss',loss)\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "writer = tf.summary.FileWriter(\"./my_model/\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for epoch in range(10000):\n",
    "        _, c, p, w11, w12, b11, b12, ls,test = sess.run([optimizer, loss, pred, W1, W2, b1, b2, loss_summary,x], feed_dict={x: X, y: Y})\n",
    "        writer.add_summary(ls,epoch)\n",
    "print (p, y)\n",
    "print(p.shape)\n",
    "\n",
    "### For the schema of the graph it will be in the zip as filename Q2 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the weights of your model\n",
    "And give an interpretation on what they are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 =  [[ 10.35075188  14.44430351]\n",
      " [ 10.40537357  13.86432362]]\n",
      "W2 =  [[-25.05777359]\n",
      " [ 24.09098434]]\n"
     ]
    }
   ],
   "source": [
    "### Code here\n",
    "\n",
    "print(\"W1 = \",w11)\n",
    "print(\"W2 = \",w12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Build a CNN to predict the MNIST digits\n",
    "You can now move to CNNs. You'll have to train a convolutional neural network to predict the digits from MNIST.\n",
    "\n",
    "You might want to reuse some pieces of code from [SNN](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/Intro_to_SNN.ipynb)\n",
    "\n",
    "Your model should have 3 layers:\n",
    "- 1st layer : 6 convolutional kernels with shape (3,3)\n",
    "- 2nd layer : 6 convolutional kernels with shape (3,3)\n",
    "- 3rd layer : Softmax layer\n",
    "\n",
    "Train your model.\n",
    "\n",
    "Explain all you do, and why, make it lovely to read, plz o:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### We build most of our code based on this video tutorial: https://www.youtube.com/watch?v=HMcx-zY8JSg ###\n",
    "### We also used the official Tensorflow documentation on the subject: https://www.tensorflow.org/get_started/mnist/pros \n",
    "### and https://www.tensorflow.org/tutorials/layers ###\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Initialisation: Import Variables\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#Import the library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "#####################################################\n",
    "\n",
    "##Creation of all objects for our Tensor flow graph##\n",
    "\n",
    "#####################################################\n",
    "# Convolutional Layer 1.\n",
    "filter_size1 = 3          # Convolution filters are 3 x 3 pixels.\n",
    "num_filters1 = 6         # There are 6 of these filters (or convolutional kernel).\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 3          # Convolution filters are 3 x 3 pixels.\n",
    "num_filters2 = 6         # There are 6 of these filters (or convolutional kernel).\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer.\n",
    "\n",
    "#Import the MNIST database. We supposed that we were alowed to use it instead of the technic shown in SNN\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True) #one_hot to get data in a vector of 10 elements, the number of possible classes (0 except 1 for the correct value) \n",
    "\n",
    "\n",
    "#Here we get data as regular number (From 0 to 9), it will be required later on\n",
    "data.test.cls = np.argmax(data.test.labels, axis=1)\n",
    "\n",
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale. So just a weight of one color so 1 channel\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits. (0 -> 9)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Creation of the model Tensorflow\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#The 2 function below are used to create weights and biaises\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    #We try to make them slightly positive to avoid \"dead neurons\" (too low bias are on the slope that don't change of RELU, 0 )\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "\n",
    "\n",
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "    \n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "        \n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights\n",
    "\n",
    "\n",
    "#To change the 4 dimentional output of a convolutional layer into a 2 dimensionnal layer Tensor\n",
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features\n",
    "\n",
    "\n",
    "#Get a fully connected layer and output a fully connected layer of an other size after done a E step\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "#Placeholder variables, that will be used to input data to the graph\n",
    "#None for arbitrarily number of image (that we will get by batch)\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "\n",
    "#We reshape the image from 1 dimension vector to a 4 dimension tensor to be able to use it in our functions\n",
    "#-1 is the number of images, here -1 to let tensorflow choose it automaticcaly\n",
    "#Then just the dimension of the image and the number of channels\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "#None as for x for the same reasons\n",
    "#It represent the true label expected for each image\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "\n",
    "#Here to get the brut value of the image (like 7)\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "\n",
    "#Convolutionnal 1\n",
    "#It takes some images as input, 1 channel, here we want 6 filters and we pool to reduce image by 2\n",
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)\n",
    "#We get from it the convolutionnal layer (here 14*14 cause we pool it by 2*2) in 6 channels\n",
    "#We get also the associated weights from it    \n",
    "\n",
    "#Convolutionnal 2\n",
    "#Same that for the 1 but we take here the convolutionnal layer 1 as input and the filter size ans num_filter that we want for this one (But we want the same that for the 1 here)\n",
    "#We pool 2*2 here too so we are supposed to get a 7*7 (14/2) size for each image by 6 filter as we have in input\n",
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)\n",
    "\n",
    "#We get the flat_layer (image of a size arbitrary given in a multiple of 7*7 in our case)\n",
    "#We get the number of features (this number multiple of 7*7)\n",
    "layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "\n",
    "#Then we do the neurol network layer reduction from this number to 128 neurons (that we fixed arbitrarily)\n",
    "#Classique one\n",
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)\n",
    "\n",
    "#And we do it again to pass from the 128 neurons layer to the 10 neurons layer that are the 10 class possibles\n",
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)\n",
    "\n",
    "#Predicted class\n",
    "#We softmax to have more easily a number between 0 and 1 and the sum of pred = 1\n",
    "y_pred = tf.nn.softmax(layer_fc2)\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "\n",
    "\n",
    "\n",
    "#We want to know how well our model performs (between pred and the value espected)\n",
    "\n",
    "#Cost function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)\n",
    "#Here we get the get a single value for all images (the average of the cross-entropy)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#Optimization\n",
    "#We use AdamOptimizer (An advenced form of Gradient descent) because it is the one the more efficient\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "#We compare the class predicted to the true class for each image\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "\n",
    "#We change this boolean as a float32 and we calculate the accuracy (True becomes 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before optimization:\n",
      "Accuracy on Test-Set: 8.3% (831 / 10000)\n",
      "Beginning of optimization:\n",
      "Optimization Iteration:      1, Training Accuracy:   7.8%\n",
      "Optimization Iteration:    101, Training Accuracy:  17.2%\n",
      "Optimization Iteration:    201, Training Accuracy:  17.2%\n",
      "Optimization Iteration:    301, Training Accuracy:  34.4%\n",
      "Optimization Iteration:    401, Training Accuracy:  71.9%\n",
      "Optimization Iteration:    501, Training Accuracy:  64.1%\n",
      "Optimization Iteration:    601, Training Accuracy:  85.9%\n",
      "Optimization Iteration:    701, Training Accuracy:  87.5%\n",
      "Optimization Iteration:    801, Training Accuracy:  85.9%\n",
      "Optimization Iteration:    901, Training Accuracy:  89.1%\n",
      "Time usage: 0:00:22\n",
      "Accuracy on Test-Set: 87.3% (8732 / 10000)\n",
      "Total number of iterations: 1000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Run the code part, see if our model performs well\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())#Initialisation of variables\n",
    "train_batch_size = 64 #Create small batches\n",
    "\n",
    "\n",
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "w21=0\n",
    "w22=0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "    global w21\n",
    "    global w22\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch = data.train.next_batch(train_batch_size)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "    \n",
    "            \n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        _, w21, w22 = session.run([optimizer, weights_conv1, weights_conv2], feed_dict=feed_dict_train)\n",
    "                    \n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "    \n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    " \n",
    "   \n",
    "    \n",
    "# Split the test-set into smaller batches of this size.\n",
    "test_batch_size = 256\n",
    "\n",
    "\n",
    "#At first try we are suppose to get around 10% accurency (10 class possibles)\n",
    "def print_test_accuracy():\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.test.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.test.images[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.test.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images,\n",
    "                     y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Convenience variable for the true class-numbers of the test-set.\n",
    "    cls_true = data.test.cls\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "        \n",
    "        \n",
    "print(\"Accuracy before optimization:\")\n",
    "print_test_accuracy()\n",
    "print(\"Beginning of optimization:\")\n",
    "optimize(num_iterations=1000)\n",
    "#We check how good worked our model at the end\n",
    "print_test_accuracy()\n",
    "\n",
    "print(\"Total number of iterations:\" ,total_iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the weights of your model\n",
    "And give an interpretation on what they are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 final=  [[[[ 0.02322254 -0.13199347  0.11424304 -0.10021503 -0.16074722  0.15149316]]\n",
      "\n",
      "  [[-0.13528076 -0.09832245  0.15406279 -0.09435652 -0.14497732  0.23707998]]\n",
      "\n",
      "  [[-0.20076568 -0.08431778  0.0366116   0.13149519 -0.11575373  0.10810126]]]\n",
      "\n",
      "\n",
      " [[[-0.10864274 -0.14398238  0.11880873 -0.1684085  -0.02598211  0.0980565 ]]\n",
      "\n",
      "  [[-0.11406117 -0.14777376  0.18402368 -0.15040454 -0.07776146  0.06292378]]\n",
      "\n",
      "  [[-0.02536114 -0.14627436  0.19902301  0.06043787  0.01435748  0.17416094]]]\n",
      "\n",
      "\n",
      " [[[-0.08711937  0.05389575  0.07869607 -0.10041837 -0.12844525  0.17155471]]\n",
      "\n",
      "  [[-0.05172232  0.05615001  0.15589829 -0.16433008 -0.0881496   0.15597637]]\n",
      "\n",
      "  [[ 0.13591775  0.00368357  0.1641943   0.06671584  0.13845287  0.12545973]]]]\n",
      "W2 final=  [[[[ 0.07559912  0.07876186 -0.09142206  0.05172145  0.09001105  0.1232834 ]\n",
      "   [ 0.09896825  0.11854528 -0.14095977  0.01333667  0.14994767  0.07939031]\n",
      "   [-0.08560187 -0.08549218  0.1299445   0.02792946 -0.13330914 -0.02491346]\n",
      "   [ 0.13821031  0.05058189 -0.13264626  0.14942192  0.08126593  0.17452434]\n",
      "   [ 0.09926223  0.0765024  -0.12298638  0.05136464  0.15751724  0.1182208 ]\n",
      "   [-0.13313088 -0.14012374  0.16380498  0.13678318 -0.00331966  0.04817297]]\n",
      "\n",
      "  [[-0.02871853  0.04272919  0.02412462 -0.03372056  0.04949472  0.02444106]\n",
      "   [ 0.06263143  0.18195467 -0.03604002  0.0525164   0.11687488  0.12881351]\n",
      "   [-0.01893652 -0.12807126 -0.01832771  0.07501809 -0.13278353 -0.10333893]\n",
      "   [ 0.09376916  0.14843205 -0.02416485  0.0709445   0.12265227  0.0631166 ]\n",
      "   [ 0.11280966  0.1661821   0.0377464   0.11029627  0.11165292  0.09174125]\n",
      "   [-0.10409557 -0.22644514  0.11375913  0.14360891 -0.10607651 -0.00556221]]\n",
      "\n",
      "  [[-0.04739883  0.16961335  0.16163772 -0.01399177  0.14008306  0.10178935]\n",
      "   [-0.08881271  0.12715016  0.09362425 -0.00393728  0.12117496  0.05853637]\n",
      "   [ 0.15073717 -0.02344675 -0.13258071  0.12811728  0.03620761 -0.14799459]\n",
      "   [ 0.14037597  0.10339455  0.04496437  0.08141988  0.10676876  0.13691543]\n",
      "   [ 0.10442152  0.13427168  0.11756112 -0.03293835  0.01993702  0.04819573]\n",
      "   [ 0.11712474 -0.15686198 -0.10502271  0.17197049  0.04023611 -0.10107134]]]\n",
      "\n",
      "\n",
      " [[[ 0.15076558  0.11882936 -0.1226273   0.18212058  0.14837082  0.16905312]\n",
      "   [ 0.10364625 -0.01836558 -0.12197494  0.08512642  0.12171517  0.15601198]\n",
      "   [-0.15514483  0.06161282  0.18232788 -0.07253814 -0.13130447 -0.10476556]\n",
      "   [ 0.05087296  0.01518646 -0.08100462  0.13939086  0.09788121  0.06685651]\n",
      "   [ 0.07305378  0.04417144 -0.13553664  0.05606352  0.09018395  0.14028193]\n",
      "   [-0.13560204  0.0114756   0.04582391 -0.11070472 -0.13083678 -0.14321321]]\n",
      "\n",
      "  [[ 0.04147649  0.04485977  0.16762958  0.04390984  0.16620094  0.15714222]\n",
      "   [ 0.02916325  0.09115769  0.04156392  0.10575919  0.0712693   0.15029471]\n",
      "   [ 0.07931724  0.11472694  0.05834853 -0.08222356 -0.19458956 -0.13014139]\n",
      "   [ 0.17572533  0.01172763  0.05261158  0.14984491  0.13224339  0.1654398 ]\n",
      "   [ 0.06188034  0.06394494 -0.01241858  0.11774074  0.14682284  0.08229268]\n",
      "   [ 0.01909702  0.07724116  0.07929295 -0.10760107 -0.16693585 -0.13354625]]\n",
      "\n",
      "  [[-0.1285767   0.09478316  0.09419953  0.0568148   0.0885796   0.0685294 ]\n",
      "   [-0.05262116  0.15538429  0.12156448  0.07922268  0.04126702  0.13231057]\n",
      "   [ 0.17748222  0.0972004  -0.11571883 -0.17983669  0.0467603  -0.20108643]\n",
      "   [ 0.02449084 -0.0070892   0.04872024  0.12399876  0.15435965  0.04319958]\n",
      "   [-0.07208347  0.05305427  0.04801595  0.17661925  0.17994593  0.1221072 ]\n",
      "   [ 0.17401014  0.1139956  -0.14635231 -0.12144967  0.04720366 -0.10723683]]]\n",
      "\n",
      "\n",
      " [[[ 0.10040411  0.06732599 -0.07311471  0.05629493  0.12596673  0.16995008]\n",
      "   [ 0.17677733 -0.02176791 -0.16749473  0.17322576  0.06847487  0.13153957]\n",
      "   [-0.10324068  0.13594764  0.07479692 -0.14303163 -0.13378344 -0.0198493 ]\n",
      "   [ 0.08727318 -0.11623734 -0.09773095  0.12563108  0.15595396  0.10885582]\n",
      "   [ 0.11490598  0.06150166 -0.06728896  0.15635921  0.13831732  0.15355456]\n",
      "   [-0.15496773  0.12210252  0.13233414 -0.1413274  -0.12478682 -0.09835217]]\n",
      "\n",
      "  [[ 0.06881058 -0.03151901 -0.00925086  0.14355104  0.08572415  0.09215412]\n",
      "   [ 0.05802613 -0.138073   -0.00049284  0.08147554  0.07366783  0.13268806]\n",
      "   [ 0.0374218   0.14946917  0.0899281  -0.13098904 -0.06842843 -0.00645647]\n",
      "   [ 0.16700001 -0.12702523 -0.05617604  0.08980189  0.09537963  0.12255607]\n",
      "   [ 0.09614942 -0.02934483  0.01443148  0.11372334  0.11319275  0.18907154]\n",
      "   [ 0.06354112  0.15113996  0.15962212 -0.184378   -0.10288222 -0.02848491]]\n",
      "\n",
      "  [[-0.10983131 -0.19224381  0.06812322  0.16887704  0.0864191   0.15734011]\n",
      "   [-0.13583626 -0.20688242  0.1116053   0.14644705  0.00558748  0.11074079]\n",
      "   [ 0.14947884  0.17728776 -0.01931086 -0.13684842  0.18663883  0.05842953]\n",
      "   [-0.01945531 -0.19388236 -0.05602846  0.11585965  0.14597403  0.10195489]\n",
      "   [-0.10531333 -0.12899369  0.08268838  0.14025548  0.1452824   0.08128316]\n",
      "   [ 0.10144794  0.03922646 -0.02705408 -0.03581543  0.14177477  0.02125841]]]]\n",
      "<tf.Variable 'Variable_8:0' shape=(3, 3, 1, 6) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_10:0' shape=(3, 3, 6, 6) dtype=float32_ref>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFirst weight is (3,3,1,6) and second one (3,3,6,6)\\nWhen we use this 6 filters filter  on the input image, the high weights will more more activ on the grey part that the weight that\\nare negativ or 0\\nSo from 1 image input, we will have 6 \"differents\" images based on the filter that were used cause each filter is diffents due \\nthe different weights that compose each filter\\n\\nIt is the same for the second weights that will take the 6 images output from the first convolutional layer (filter by weights1)\\nand filter them also by their weights and create 6 output images that are different result from the same image (same principle that for weights1)\\nIn theory they are supposed to represent the relations between each pixel with the adjacent one by detecting pattern in the input image\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### code here\n",
    "\n",
    "### Here you have the final weights \n",
    "print(\"W1 final= \",w21)\n",
    "print(\"W2 final= \",w22)\n",
    "\n",
    "print(weights_conv1)\n",
    "print(weights_conv2)\n",
    "\"\"\"\n",
    "First weight is (3,3,1,6) and second one (3,3,6,6)\n",
    "When we use this 6 filters filter  on the input image, the high weights will more more activ on the grey part that the weight that\n",
    "are negativ or 0\n",
    "So from 1 image input, we will have 6 \"differents\" images based on the filter that were used cause each filter is diffents due \n",
    "the different weights that compose each filter\n",
    "\n",
    "It is the same for the second weights that will take the 6 images output from the first convolutional layer (filter by weights1)\n",
    "and filter them also by their weights and create 6 output images that are different result from the same image (same principle that for weights1)\n",
    "In theory they are supposed to represent the relations between each pixel with the adjacent one by detecting pattern in the input image\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose one (tell me what you chose...)\n",
    "- Show how the gradients (show only one kernel) evolve for good and wrong prediction. (hard)\n",
    "- Initialize the kernels with values that make sense for you and show how they evolve. (easy) \n",
    "- When training is finished, show the 6+6=12 results of some convolved immages. (easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### We chose n°3 When training is finished, show the 6+6=12 results of some convolved immages. (easy) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see convolutionnal layer\n",
    "def plot_conv_layer(layer, image):\n",
    "    # Assume layer is a TensorFlow op that outputs a 4-dim tensor\n",
    "    # which is the output of a convolutional layer,\n",
    "    # e.g. layer_conv1 or layer_conv2.\n",
    "\n",
    "    # Create a feed-dict containing just one image.\n",
    "    # Note that we don't need to feed y_true because it is\n",
    "    # not used in this calculation.\n",
    "    feed_dict = {x: [image]}\n",
    "\n",
    "    # Calculate and retrieve the output values of the layer\n",
    "    # when inputting that image.\n",
    "    values = session.run(layer, feed_dict=feed_dict)\n",
    "\n",
    "    # Number of filters used in the conv. layer.\n",
    "    num_filters = values.shape[3]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create figure with a grid of sub-plots.\n",
    "    fig, axes = plt.subplots(1, num_filters)\n",
    "\n",
    "    # Plot the output images of all the filters.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the images for valid filters.\n",
    "        if i<num_filters:\n",
    "            # Get the output image of using the i'th filter.\n",
    "            # See new_conv_layer() for details on the format\n",
    "            # of this 4-dim tensor.\n",
    "            img = values[0, :, :, i]\n",
    "\n",
    "            # Plot image.\n",
    "            ax.imshow(img, interpolation='nearest', cmap='binary')\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACf1JREFUeJzt3WdoVEsUB/CJJSTEiFGfPRp7Rw2WGBXBhhWjgg1RVBQN\nsYtBiIgo2Av2EhWJvWPNB0ssWBMLKFERYtRoNNGYWGPb9+HxzjtnXnazye7dnY3/36czmct1rrs7\n7D17Zq6fzWZTAADgfaW8PQAAAPgHJmQAAENgQgYAMAQmZAAAQ2BCBgAwBCZkAABDYEIGADAEJmQA\nAENgQgYAMESZohwcEBBgCw4OtmoslsvOzs622Wx/OTrG39/fFhgY6KkhuV1eXl6h11i5cmVbWFiY\nh0bkfikpKYVeo1JKBQUF2UJCQjwxJEtkZGQUep2BgYE+/ZnMyspy6rX8U96zRZqQg4OD1ZAhQ4o/\nKi/bsmVLemHHBAYGqsjISE8MxxKJiYmFXmNYWJhKTk72xHAs4efnV+g1KqVUSEiImjp1qtXDsUxs\nbGyh1xkcHKyGDh3qieFYYsOGDU69ln/KexYpCwAAQ2BCBgAwBCZkAABDYEIGADAEJmQAAEMUqcrC\nWd++fRPtXbt2UTxo0CDRV6dOHYq/fv1q95z5+fkU79mzR/TNmDGD4tzc3KIN1k34NSclJYm+Bg0a\nFBg78vPnT9FOTU2luHHjxqLP39/f2WEWye/fv0V77dq1FL969Ur0RUdHU+yoPImf8/Dhw6KvfPny\nFPfs2VP0lS5duvABF9OvX79EOy0tjeLx48eLvps3b1Ksv8/tnTM9Xf7AHhERQfHbt2+LNthi0j9b\nO3bsoHjatGmijz+0Qv+/4fhnMiEhQfTNnz+f4oyMjKIN1gX6Azf27dtH8cuXL0Ufn4saNmzo1DnP\nnTsn+n78+EGx/p4tW7asEyOW8A0ZAMAQmJABAAxhScrizJkzot2tWzeKV6xYIfri4uIo5resuuPH\nj1PctGlT0Ve1alWK379/L/qsvNXlpk+fTvHFixdFX58+fZw6B7810m8j+/btS3G/fv2KM8QiO3To\nkGjz16BixYqij9+6OcJv+U6ePCn6WrZsSXGPHj2cHqernjx5YnccHTp0EH2XLl1y6px3796luF69\neqJvzJgxFC9fvtzpcbri4MGDos2va82aNaIvJibGqXPGx8dTHBoaKvpGjRpF8dKlS50ep6suXLgg\n2uvXr6dYH2NUVJRT57x16xbF27ZtE308BdmrVy+nx2kPviEDABgCEzIAgCEwIQMAGMJtOeSzZ89S\nXL9+fdF3/vx5ivUyInt54wcPHog2LzG6f/++6JswYQLFnsoZ63nsgQMHUuzsBkx6iQ7ftat///6i\nz9l8l6t4adCBAwdE3/fv3ykePny46LNXNvTmzRvR3rx5M8X69YeHh1NcpowlP2+QZ8+eUayXds2c\nOZPi1atXO3W+zMxM0a5UqRLFS5YsEX0rV650dpguOXHiBMV6HvvGjRsUx8bGOnW+lJQU0Q4ICKD4\n+fPnoo//H1otOzub4u3bt4s+PkY9x9uoUaMCz5eTkyPaO3fupFgvdWzXrh3F7njP4hsyAIAhMCED\nABjCbfeFNWrUoJivAlJKljo5SinwVMSnT59EH19pNHLkSNHHV6p5KmWxatUq0eblNkePHhV9TZo0\nofjLly8U67dMvMyMlxQpJVdFWYnfyuslRDxNo6elXr9+TTG/feUpCv38+i1kp06dijze4jp9+jTF\nU6ZMsXuco3I+vupQX5XVqlUrinnqQCnHq9/cqVSp/75v8TI03efPn+328ZSTfruelZVF8eXLl0Uf\nT29Z7enTpxTrJad81ai+wpWvIOTXuXv3bnHcw4cPKdY/s+4odePwDRkAwBCYkAEADIEJGQDAEG7L\nIfOcmV4SxtsVKlQQfY8ePaKYL7F29Jww/aGOfn5+RRusBRwtD7969SrFfKmlo2WzR44ccePonMdf\nH70s7c6dOxR37txZ9PHyH57/50uIlZLvhREjRog+Tz5cli/91nPls2fPpnjOnDku/1vLli1z+RzF\nwUsnhw0bZve4cePGiTb/nYPnu7t27Wr3HPrSbE8qV64cxfrcwMs49ets06YNxTyPzn9fUEqWdOpl\nkEFBQcUYsX34hgwAYAhMyAAAhrBkOZS+SofvXNa2bVvRx28HFixYQLGesuC3G3ppm6dK3Ti+Mk8p\nuQPbrFmzRN+kSZMozsvLs3tOb972/at58+YUnzp1SvTxlXu8zE0ppWrVqkXx48ePKdZ3hRs8eDDF\nLVq0cG2wLujduzfFPBWjlFITJ06kuH379qKPr8biO7/x1VxK/X8jc2/Td3Tj6SKebtLpZWTctWvX\nXB+YG/Cy0kWLFom+xMREivVN+nl6gz8ooFq1auI4/tlu3bq1a4MtBL4hAwAYAhMyAIAhLElZ8F9p\nlVJq8eLFdo/lv3Dzagl9JRy/VeQrkLxFX4l07NixAmOlZLUC33iIb4qk1P9ve72BvwZdunQRfXqb\n42mK0aNHU9yxY0dx3NixYyn2RqrpXyEhIRR3797d7nH6Aw94VZCjFXd6GsTb9Pcrf84lj5WSKwt5\n9cG7d+/EcXv37nXnEIuNzw16qtNRtdaLFy8o5ilRPZXGN0SzetMr789sAACglMKEDABgDEzIAACG\nsDYh4gReFsU3zdZ34OL5OhNyyMXF88Z8435fx3f74rug6RvX165d22NjskJycjLFvJyRPwjT1/Ed\n7lJTUym+ffu2N4ZjmStXrlDMd67jK/iUsr+RvRV8d2YDAChhMCEDABjC6ykLXi7F8VsIpZSqXr06\nxfqKG9PZ23hd3+yEr+KLiIiwdEzuxp+pyF87vYSIv97eXKlXXHzjpOjoaIo3bdokjuPPctM3djcd\nf0ZdlSpVKF63bp04jq90+/jxo/UDczO+0pB/9iIjI8Vx9+7do1hfuelu+IYMAGAITMgAAIbAhAwA\nYAiv55A3btxY4N/3798v2nwHLl8re5s3b16Bf09PTxdtvuR669atlo7J3XJzcynmD4zUS/tCQ0Mp\n9sUcclpaWoF/nzx5smgnJSVR7GslcQMGDCjw7wsXLhRtnjfnDwL1Ffy3jpycHIqvX78ujqtZs6bH\nxuRbMxsAQAmGCRkAwBBeT1nwjaETEhIo1je/1p/v5kv47md8E/65c+eK4/htv6/hJWD8eXvNmjUT\nx+mroHwN370uPj6eYr4rmlJK5efne2xM7hYeHl7g3+Pi4kT7w4cPnhiOZfhDCurWrUsxL7FVyvpN\n6Tl8QwYAMAQmZAAAQ2BCBgAwhNdzyHw3KUe8+XQJV/HyGp5rLUn4k1B4XNJERUVRrD9NpKTIzMyk\nOCYmxosjsRZ/eo0p8A0ZAMAQmJABAAzhV5RyMj8/vyylVHqhB5qrjs1m+8vRAbhGn1DoNSr1Z1zn\nn3CNSv1B1+nL9b0AACUJUhYAAIbAhAwAYAhMyAAAhsCEDABgCEzIAACGwIQMAGAITMgAAIbAhAwA\nYAhMyAAAhvgb5GA54/ixxLQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb1f0eba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEAgaMgAEgoYMAIGgIQNAIP4BCQkKg9KF2bsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb180a7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image1 = data.test.images[6]\n",
    "plot_conv_layer(layer=layer_conv1, image=image1)\n",
    "plot_conv_layer(layer=layer_conv2, image=image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgCwBCZkAABL/A+Iy/b/tBpsIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb0eb9f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABmpJREFUeJzt3TtoFVsbxvGV7ZZsiYk7YoxmCwdFvBC8VhaWGrDxlgje\nKsFCEAsLtdFCKwvxghdUsFAUk8pGFFGwESwUEYkIKhqMoiQmwXuMcU7xfcUp1rP2zOzZJ+/B/69c\nT9bMLHfmzZC8rqmJosgBAMZebqwvAADwPxRkADCCggwARlCQAcAICjIAGEFBBgAjKMgAYAQFGQCM\noCADgBH5JF88adKkqLm5OfFJ0vxvwJqamsRznHPu9+/fMnv58mV/FEVNofmNjY1RqVRKfN7QGtU1\n5XL652Ha9Xd3d5ddY7FYjFpaWrxZoVCQ80JrVFlojWk9evSo7Bqdc65QKET19fWZnnvcuHHe8dHR\n0UzP45xz/f39ZddZW1sb1dXVJT72r1+/ZJbPJyoLFRkcHIz1WeZyuSjNdYU+F3WPVeN7dmRkJNY6\nE62wubnZnT59OvHFfP/+XWbqHyVUGEI+ffoks/b29p5y80ulkuvq6kp83pGREZl9+/bNOx66kdLe\nFK2trWXX2NLS4i5duuTN5s6dK+eFftj9/PnTO572cwypr68vu8b/f51bv3594uOH1tnQ0OAd//Ll\nS+LzlHPu3Lmy66yrq3NtbW2Jjz0wMCCzYrHoHa9Goers7Iz1WebzeZfmYXBoaCh4TJ+JEycmPk85\nvb29sdbJrywAwAgKMgAYQUEGACMoyABgBAUZAIzIrL/l1atXMrtw4YLMDhw44B0PtX2N1ab67969\nk9nKlSsTH+/o0aMyW7Fihcyq8dfuOMd+//69zHbt2uUd7+jokHM2btwos7Rtf3GFvodmzpwps7Vr\n13rHT5w4kepc1XT37l2Zha5p//793vF79+6lOl61ff78WWah7pdVq1Z5x588eVLxNaXFEzIAGEFB\nBgAjKMgAYAQFGQCMoCADgBEUZAAwIrO2t1OnTsns8ePHMhscHPSOb968Wc65fft2/AvL0LNnz2S2\nYcMGmfX393vHHz58KOcsW7ZMZmqDmyyE2s36+vpk1t3d7R2/ceOGnBNq7WtqKrsxVkVC62xvb5eZ\n2nxp9+7dcs6RI0fiX1iGPnz4ILNNmzbJbOfOnd7xxYsXyzknT56Mf2EZq62tldnbt29lpnY83Ldv\nn5xz+fLl+BeWAk/IAGAEBRkAjKAgA4ARFGQAMIKCDABGUJABwIjM2t5Ua5dzzi1YsEBmage13t5e\nOafaO4EpU6ZMkdm8efNkNm3aNO946N1db968kVlra6vMKjVhwgSZXb9+XWZq174dO3bIOdeuXZPZ\n9u3bZZaF0HseQ21Uypw5c2Q2VjsXTp48WWbr1q1LfLzly5fLLNT2Wu2d4BobG2WmWttC9uzZI7Mr\nV67ILIt18oQMAEZQkAHACAoyABhBQQYAIyjIAGBEZl0Wob/ahjomSqWSd3zr1q1yzsWLF2X24MED\nmVVq4cKFMgu9H07p7OyU2cDAgMy+fv2a+FxxhboPQp0Pt27d8o6HNkk6c+aMzLZs2SKzLLx+/Vpm\noXfqTZ8+3Tu+bds2OWfRokUyC228VamlS5fK7ObNmzJ7+vSpdzzULTJWa3QufD8cPHhQZj9+/PCO\nHzp0SM6ZPXu2zF68eCGzuHhCBgAjKMgAYAQFGQCMoCADgBEUZAAwgoIMAEZk1va2evVqman2Euec\nGxoa8o4XCgU5J7TJjdrIp9pUq1DIkiVLZHb//n2ZhVriqmnq1KkyW7NmjXc81CrX1tYms9C7+LKg\n3o3nnHPFYlFm6h2Qd+7ckXPOnj0rs9C9UanQ5kLqvnPOuY8fP3rHQ9cauicPHz4ss6tXr8osC+fP\nn088J7TOvXv3yuz48eMyC20W9k88IQOAERRkADCCggwARlCQAcAICjIAGEFBBgAjMmt7C+0EFXpP\nm8pmzZol54RaSGbMmCGzOIaHh11PT09Fx8hCR0eHzJ4/f16186Ztwxo/frx3XLWJORduexseHk51\nHXHlcvpZRO3oFspC3zPHjh2TWWgHwUqF7kn1eYWy0dFROaerq0tm+XxmZeZfEWphnD9/vswaGhoq\nPjdPyABgBAUZAIygIAOAERRkADCCggwARlCQAcCImiiK4n9xTU2fc27se8LS+yuKoqbQF7DG/4Sy\na3Tuz1jnn7BG5/6gdSYpyACA6uFXFgBgBAUZAIygIAOAERRkADCCggwARlCQAcAICjIAGEFBBgAj\nKMgAYMTfBqzHCbIXdQcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb149c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image2 = data.test.images[7]\n",
    "plot_conv_layer(layer=layer_conv1, image=image2)\n",
    "plot_conv_layer(layer=layer_conv2, image=image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wCd+yPa0VjXPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb187ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA4AnaMgA4Ik/AZ6ca1Kil8qEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb2172470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image3 = data.test.images[8]\n",
    "plot_conv_layer(layer=layer_conv1, image=image3)\n",
    "plot_conv_layer(layer=layer_conv2, image=image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nALAEJmQAAEtgQgYAsAQmZAAAS/wPwiEJM0euXTwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb1917860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb13c27b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image4 = data.test.images[9]\n",
    "plot_conv_layer(layer=layer_conv1, image=image4)\n",
    "plot_conv_layer(layer=layer_conv2, image=image4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACjlJREFUeJzt3WlsTN8bB/BTdNFqotXaammsDRJrLLU1hFpiJ6Kk9iVS\nvKIREQmJCEJCUlEhgoQSRe37EltDYxchLaW1ltp35vfi/8+T5zzMdOb23pmD7+fVc53rzrmd29M7\nz5z7nCCXy6UAACDwKgS6AwAA8D8YkAEADIEBGQDAEBiQAQAMgQEZAMAQGJABAAyBARkAwBAYkAEA\nDIEBGQDAEJV82Tk0NNQVERHhVF8cV1paWuJyuWI97RMSEuIKDw/3V5ds9+bNmzLPMSYmxhUfH++n\nHtkvLy+vzHNUSqmIiAhXVFSUP7rkiOLi4jLPMywszBUZGemvLtmupKTEq/fyX7lmfRqQIyIiVO/e\nva33KsCysrIKy9onPDxcde/e3R/dcUROTk6Z5xgfH68uX77sj+44IigoqMxzVEqpqKgoNXPmTKe7\n45j09PQyzzMyMlINHTrUH91xRGZmplfv5b9yzSJlAQBgCJ/ukO1QWKj/oXj58iXF79+/p7hbt25+\n65Pd3r17p23zAk65ubkUV6ig/z2sW7cuxU2aNHGod/b49OmTtv327VuKHz58SHFISIi2X/369Smu\nWrWqQ72zz8ePH7XtsLAwiuX796d69OiRts3fy+DgYIpNvybL8vnzZ237w4cPFN+9e5diWXCNp0pq\n167tTOf+7++4ogAA/gIYkAEADIEBGQDAEH7JIWdlZVHMc3BKKTVt2jSKu3TpQrHM1Tx+/Jji8+fP\na23FxcW29LM8vn37RvHNmze1Nj5VcOPGjRTn5+dr+y1cuJBimddLTEykuHLlyuXqq1WbN2+mePXq\n1W73e/HiBcWVKumXWMWKFSlOT0/X2lJTU3+7n78VFRVR3LFjR60tJSXF5+Px91wppZ4/f26pX3bK\nzs6mODZWn401f/58iuPi4iiuUaOGth/Pr584cUJr4znZQNq7dy/Fa9eu1dp+/vxJcUFBAcVyjIqO\njqZ41KhRWtvYsWMplt+XWIE7ZAAAQ2BABgAwhCMpi6dPn7ptk9OlrBg2bJi2zT/qfv36tdzHt4JP\nzj9w4IClY8ydO5fikydPam08neGvlMXr16+17czMTIpluqVhw4YUJyUlUSyvhUOHDlE8Y8YMra15\n8+YUt2/f3vcOWyTP88yZMxSvWrVKa+NT+vLy8ii+dOmSth9PuaWlpWltK1asoPj79+8Weuw7mQIr\nKSmhmKeYrGrdurW2PWvWLIrldDMn8amzSimVkZFB8ZEjR7Q2Pp2tVatWFJeWlmr7Xbx4kWKZjuTT\nOO14aA53yAAAhsCADABgCAzIAACGcCSHzKeoKaXUnTt3KJbTYWbPnk0xn24SFBSk7Td58mSKe/bs\nqbX169eP4t27d1vocflNmjTJbdugQYO8OgbPoS5evFhr27RpE8WvXr3ysXfWPHnyRNv+8uULxc2a\nNdPa1qxZQ3FCQoLbY06dOpXiXbt2aW07d+6k2J855OvXr2vb/NFhadu2bb/9d3nN86luPKeulFJ9\n+/almE/LctKNGze07Tlz5rjdl79HnnTo0IHiCRMmaG38mufTXp0mv7PgUwwbNGigtW3ZsoVingOX\n30PxqbnHjh3T2nJycihGDhkA4C+CARkAwBCOpCxkhaymTZtSLD8OeTuFa926dRTLlEWnTp0oDlTK\nguMfhXxx69Ytt21LliyheMqUKZaO7ys59Y4XQu/Tp4/W1qJFC6+OyadD7du3T2uTU7P8RaYo+BTD\n48ePe3UMmcI5fPgwxTI9wKdF+itlIT+GL1iwgOKtW7daOiavXNi4cWOtrUePHhT7M2Vx+vRpbZuP\nL7xPSukpF04+ccevWflE4oMHD6x00y3cIQMAGAIDMgCAITAgAwAYwpEcMq/EL8nHNO2okBQInqay\n7dixw/bXk5W2/OHChQvaNq/AZnVFF75KSLVq1ax1zGb37t3TtseMGUPx0qVLLR2Tf68hpw8Ggnw8\nnC/ke+rUKUvH5L/n9+/f19q6du1q6ZjlxR9nl5KTky0dk//uxcTEWDqGt3CHDABgCAzIAACGsC1l\ncfXqVYpXrlyptfEn9azylAaRixL6w7hx47TtQFWZsxv/aMsXoFVKX4RVVvfyFn8f/VXp7Hf4046e\nqhNaJRe6DQT+O9mrVy/bj8+furXjKTWreIU3OXWyevXqFPMFMHzBq9XJanJ2wx0yAIAhMCADABjC\ntpQF//bcU2EYb2dVyG+mZWFoThbi8YcKFfz7t4wXRncST1PI1+Qf/7x9H3lBIqX0JylDQ0O1Nqe/\nweauXbtGsdUZI5xceOHHjx8Uy0L8hYWF5X49X/HCVVbJ30m+zWemKPVrsSUnPXv2jGL5s23UqBHF\ncq08d/j6mEoptX//for5OnxK2T9TCHfIAACGwIAMAGAIDMgAAIawLYfMFwyUla+8nfZ29OhRimXF\nuMTERLf/j+c9g4ODvXotu/GpMTIHxfvEp0PJfJecLsjxilNO4n2tVMn95SGnGvIFBYqKiiiWRej3\n7NlDcZUqVbS24cOH+9bZcuA5cFm83Vv8PZc50wEDBlCckpKitS1btszS6/mKL9zJKy764ty5cxTz\nXK1SeoU3+Z3KokWLLL2eFfw6ld9L8OuS5/WV0p885ee2fft2bT++2ICsTjlixAjfO+wB7pABAAyB\nARkAwBCOFBe6ffu2ts2fivI0Xaxly5YUz5s3T2vjRelHjhxZ3i7ajqcfDh486Ha/mjVrUpydna21\n8YI0fK1BpX6dbuOUOnXqUCyn9PDi3IMHD9ba+MfGgoICiuVH+ejoaIqnT5+utfHzdxqfDiUL5XOj\nR49221ZaWkqxpwL9GzZs0LYD8WSpXMuSa9Omjdu22NhYinkaRim9wPvy5cu1Nn9dr0opVatWLYpl\nEa78/HyK5bqX/JrlP58rV65o+/Hf2SFDhmhtdj8BiTtkAABDYEAGADAEBmQAAEM4kkOWCwGmpaVR\nbLXi0sCBAymWeWhvH4m0k8x/ZmRkUGw1R8jzcJ5yfk7iP9vx48drbfwR4JycHLfH4IuhysUv+TFl\nftafUxZ5rrFt27Za28SJEylev36922PExcW5beMLe5aUlFjpoq14aQNJLjzsLb4Qgyzy7098CqNc\nfJcvMMCnr0n82uNTeJVSKjU1lWK5wLCcZldeuEMGADAEBmQAAEM4krI4e/asx21vyOpZfA0wExQX\nF2vbntbY+1PJCl7t2rWjWBYC5/gTUHx6mVJK1atXz6be2ad///7adm5uLsVW19QzjVzzzmqawnQy\nlchTZp6eGObjS+fOnbU2+eSxk3CHDABgCAzIAACGwIAMAGAIR3LIdpBVlSDwEhISfhv/6eS1lpSU\nFJiOQLnJKbHJycm/jU2FO2QAAENgQAYAMESQL0+VBQUFvVBK+X+FRvvUd7lcsZ52wDn+Eco8R6X+\njfP8F85RqX/oPANRChAAAH6FlAUAgCEwIAMAGAIDMgCAITAgAwAYAgMyAIAhMCADABgCAzIAgCEw\nIAMAGAIDMgCAIf4DsDgyaD/tbGcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cac9704e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAQNGQACQUMGgEDQkAEgEH8CNz1XZnsHwFMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb3fe5c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image5 = data.test.images[10]\n",
    "plot_conv_layer(layer=layer_conv1, image=image5)\n",
    "plot_conv_layer(layer=layer_conv2, image=image5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cb0c6b5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQCBoyAAQiL8A9w/2hUDbwrYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26cacd42278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image6 = data.test.images[11]\n",
    "plot_conv_layer(layer=layer_conv1, image=image6)\n",
    "plot_conv_layer(layer=layer_conv2, image=image6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
