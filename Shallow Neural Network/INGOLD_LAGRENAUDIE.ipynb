{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to Shallow Neural Networks\n",
    "\n",
    "In this tutorial, you'll implement a shallow neural network to classify digits ranging from 0 to 9. The dataset you'll use is quite famous, it's called 'MNIST' http://yann.lecun.com/exdb/mnist/. A French guy put it up, he's very famous in the DL comunity, he's called Yann Lecun and is now both head of the Facebook AI reseach program and head of something in the University of New York...\n",
    "\n",
    "\n",
    "###Â First step\n",
    "\n",
    "As a first step, I invite you to discover what is MNIST. You might find [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/dataset_MNIST.ipynb) to be usefull, but feel to browse the web.\n",
    "\n",
    "Once you get the idea, you can download the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, gzip, numpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")\n",
    "f.close()\n",
    "\n",
    "\n",
    "def to_one_hot(y, n_classes=10): # You might want to use this as some point...\n",
    "    _y = np.zeros((len(y), n_classes))\n",
    "    _y[np.arange(len(y)), y] = 1\n",
    "    return _y\n",
    "\n",
    "#We use the function to_one_hot on the y part of our dataset to get the numeric value of y_set as a vector which has\n",
    "#a 1 at his index corresponding to his value to be able to work on it\n",
    "\n",
    "X_train, y_train = train_set[0], to_one_hot(train_set[1])\n",
    "X_valid, y_valid = valid_set[0], to_one_hot(valid_set[1])\n",
    "X_test,  y_test  = test_set[0], to_one_hot(test_set[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# You can now implement a 2 layers NN\n",
    "\n",
    "Now that you have the data, you can build the a shallow neural network (SNN). I expect your SNN to have two layers. \n",
    "    - Layer 1 has 20 neurons with a sigmoid activation\n",
    "    - Layer 2 has 10 neurons with a softmax activation\n",
    "    - Loss is Negative Log Likelihood (wich is also the cross entropy)\n",
    "    \n",
    "You'll need to comment your work such that I understand that you understand what you are doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HELPER \n",
    "def softmax(Z):\n",
    "    \"\"\"Z is a vector eg. [1,2,3]\n",
    "    return: the vector softmax(Z) eg. [.09, .24, .67]\n",
    "    \"\"\"\n",
    "    #classical softmax function based on the formula\n",
    "    return np.exp(Z) / np.sum(np.exp(Z))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    #classical sigmoid function based on the formula\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "\n",
    "# Define the variables here (initialize the weights with the np.random.normal module):\n",
    "\n",
    "#We create the weight of the first hidden layer of 784 (input layer number) by 20 (the size of layer 1 neurons)\n",
    "W1 = np.random.normal(0,0.1, (784,20))\n",
    "#We define b1, with 20 row corresponding for the 20 neurons of the first hiden layer\n",
    "#The bias is really important in Neural network, it is used to smooth the sum of neuron*weight to avoid\n",
    "#every points to be at 1 or 0 (really common with sigmoid function)\n",
    "b1 = np.zeros((20,))\n",
    "\n",
    "#We create the weight of the second hidden layer of 20 (input neuron from the hidden layer 1) by 20 (the size of layer 1 neurons)\n",
    "W2 = np.random.normal(0,0.1, (20,10))\n",
    "#We define b2, with 10 row corresponding for the 10 neurons of the second hiden layer\n",
    "b2 = np.zeros((10,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pred(X, W1, b1, W2, b2 ):\n",
    "    \"\"\"Explanations ...\n",
    "    Arguments:\n",
    "        X: An input image (as a vector)(shape is <784,1>)\n",
    "        W1: Weights for hidden layer 1\n",
    "        b1: bias for hidden layer 1\n",
    "        W2: Weights for hidden layer 2\n",
    "        b2: bias for hidden layer 2\n",
    "        \n",
    "        Here we took all the the image <50000 rows, each image,784 column, each point of the image>\n",
    "        Then we calculate the prediction of our algorithm base on the formula seen in class and the data given\n",
    "        (sigmoid to get A2 and softmax to get A3)\n",
    "        It give in P the probability to be each digit (0->10) for the image corresponding to the row\n",
    "        That is way P is a <50000,10> matrix\n",
    "    Returns : a vector P\n",
    "    \"\"\"\n",
    "\n",
    "    A1 = np.dot(X, W1) +b1\n",
    "    A2 = sigmoid(A1)\n",
    "    A3 = np.dot(A2, W2) +b2\n",
    "    P = softmax(A3)\n",
    "    \n",
    "    return P\n",
    "\n",
    "def loss(P, Y):\n",
    "    \"\"\"Explanations : \n",
    "    Arguments:\n",
    "        P: The prediction vector corresponding to an image (X^s)\n",
    "        Y: The ground truth of an image\n",
    "        \n",
    "        Here we use the formula of the loss seen in class\n",
    "    Returns: a vector ???\n",
    "    Not a vector but the sum of the negative log likelihood\n",
    "    It represent how far our model is from the real data, we try to reduce it\n",
    "    \"\"\"\n",
    "    return -np.sum(np.multiply(Y, np.log(P)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Define Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dW1(P,Y,W2,W1,b1,X):\n",
    "    \"\"\"Explanations ??\n",
    "    It is the derivate of the weight for hidden layer 1\n",
    "    It is use to update the weight regarding how far the result of the sum weight + bias is far from expectation\n",
    "    We determinate it by derivating the loss function\n",
    "    (This function didn't work but i put the mathematic expression that we tried to build in Python:\n",
    "    dW1 = (P-Y)*W2.T*[A2*(1-A2)]*X)\n",
    "    Returns: A vector which is the derivative of the loss with respect to W1\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    This is were we stop to try to make the function correctly working:\n",
    "    \n",
    "    A1 = np.dot(X, W1) +b1\n",
    "    A2 = sigmoid(A1)\n",
    "    \n",
    "    part_1=np.dot((1-A2).T,A2) ##20x20\n",
    "    part_2=np.dot(W2.T,part_1) ##10x20\n",
    "    part_3=np.dot((P-Y),part_2) #50000x20\n",
    "    part_4=np.dot(X.T,part_3) #784x20\n",
    "    return part_4\n",
    "    \n",
    "    When we derivated the loss according to a variable, we tried to get back with the same variable's dimension. \n",
    "    We managed to do it for W1,W2 and b2 but not for b1\n",
    "    \"\"\"\n",
    "\n",
    "    A1 = np.dot(X, W1) +b1\n",
    "    A2 = sigmoid(A1)\n",
    "    \n",
    "    part_1=np.dot((1-A2).T,A2) ##20x20\n",
    "    part_2=np.dot(W2.T,part_1) ##10x20\n",
    "    part_3=np.dot((P-Y),part_2) #50000x20\n",
    "    part_4=np.dot(X.T,part_3) #784x20\n",
    "    return part_4\n",
    "\n",
    "\n",
    "def db1(P,Y,W2,W1,X,b1):\n",
    "    \"\"\"Explanations ??\n",
    "    It is the derivate of the bias for hidden layer 1\n",
    "    It is use to update the bias regarding how far the result of the sum weight + bias is far from expectation\n",
    "    We determinate it by derivating the loss function\n",
    "    (This function didn't work but i put the mathematic expression that we tried to build in Python:\n",
    "    dW1 = (P-Y)*W2.T*[A2*(1-A2)]    same as before except dA1/db1 = 1)\n",
    "    Returns: A scalar which is the derivative of the Loss with respect to b1\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    This is were we stop to try to make the function correctly working:\n",
    "    \n",
    "    A1 = np.dot(X, W1) +b1\n",
    "    A2 = sigmoid(A1)\n",
    "    part1=np.dot((1-A2).T,(P-y_train)) \n",
    "    part2=np.dot(part1,W2.T) \n",
    "    part3=np.dot(A2,part2) \n",
    "    return part3.T\n",
    "    \n",
    "    When we derivated the loss according to a variable, we tried to get back with the same variable's dimension. \n",
    "    We managed to do it for W1,W2 and b2 but not for b1\n",
    "    \"\"\"\n",
    "    \n",
    "    A1 = np.dot(X, W1) +b1\n",
    "    A2 = sigmoid(A1)\n",
    "    return (P-Y)*W2*A2(1-A2)\n",
    "\n",
    "\n",
    "def dW2(P, Y, W1, X, b1):\n",
    "    \"\"\"\n",
    "    Same as dW1 but for the second hidden layer\n",
    "    \"\"\"\n",
    "    A1 = np.dot(X, W1) +b1\n",
    "    A2 = sigmoid(A1)\n",
    "    return np.dot(A2.T,(P-Y))\n",
    "\n",
    "\n",
    "def db2(P,Y):\n",
    "    \"\"\"\n",
    "    Same as db1 but for the second hidden layer\n",
    "    \"\"\"\n",
    "    return np.sum(P-Y,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Train you model\n",
    "\n",
    "You may use Standard Gradient Descent (SGD) to train your model. (Experiment with many learning rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (50000,10) (20,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e76ef303dbda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mW1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdW1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdb1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mW2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdW2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-25e7f217366e>\u001b[0m in \u001b[0;36mdb1\u001b[1;34m(P, Y, W2, W1, X, b1)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (50000,10) (20,10) "
     ]
    }
   ],
   "source": [
    "#We set a learning rate\n",
    "\n",
    "alpha = 0.0001\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "    #Forward\n",
    "    P = Pred(X_train, W1, b1, W2, b2)\n",
    "    \n",
    "    #We check how for our prediction is from real output\n",
    "    test = loss(P,y_train)/len(X_train)\n",
    "    \n",
    "    \n",
    "    #Backward\n",
    "\n",
    "    #We update all the parameters\n",
    "    \n",
    "    W1 = W1 - alpha*dW1(P,y_train,W2,W1,b1,X_train)   \n",
    "    b1 = b1 - alpha*db1(P,y_train,W2,W1,X_train,b1)\n",
    "\n",
    "    W2 = W2 - alpha*dW2(P,y_train,W1,X_train,b1)\n",
    "    b2 = b2 - alpha*db2(P,y_train)\n",
    "    \n",
    "    #We update the loss with our new parameters to check if out model reduce his loss by iterations\n",
    "    loss2 = loss(Pred(X_train, W1, b1, W2, b2), y_train)/len(X_train)\n",
    "    print(\"Loss\" , loss2)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Test the accuracy of your model on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# You can now go Deeper\n",
    "\n",
    "Build a deeper model trained with SGD (You don't need to use the biases here)\n",
    "    - Layer 1 has 10 neurons with a sigmoid activation\n",
    "    - Layer 2 has 10 neurons with a sigmoid activation\n",
    "    - Layer 3 has 10 neurons with a sigmoid activation\n",
    "    - Layer 4 has 10 neurons with a sigmoid activation\n",
    "    - Layer 5 has 10 neurons with a sigmoid activation\n",
    "    - Layer 6 has 10 neurons with a softmax activation\n",
    "    - Loss is Negative Log Likelihood\n",
    "\n",
    "Is it converging ? Why ? What's wrong ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
