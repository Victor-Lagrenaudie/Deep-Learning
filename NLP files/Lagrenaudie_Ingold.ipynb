{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with RNN\n",
    "\n",
    "In this lab, we are going to generate text with RNNs.\n",
    "\n",
    "We'll try to have a RNN learning the *fables de la Fontaine*.\n",
    "\n",
    "Lets load into variable the *Fables*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./fables.txt',encoding=\"utf-8\") as f:\n",
    "    \n",
    "    \n",
    "\n",
    "    text = f.read()    #We remove the space, the tabulation etc to reduce the chance go get too many \n",
    "                       # spaces as precition outputs\n",
    "    text=text.replace(\"  \",\" \")\n",
    "    text=text.replace(\"   \",\" \")\n",
    "    text=text.replace(\"    \",\" \")\n",
    "    text=text.replace(\"      \",\" \")\n",
    "    text=text.replace(\"       \",\" \")\n",
    "    text=text.replace('\\n \\n','\\n')\n",
    "    text=text.replace('\\n',\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "\n",
    "Define some methods to read this text\n",
    "- a batch generator, generating batchs of text\n",
    "- a decoder to translate a batch into stg more convinient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 90 different elements in  my text which are :\n",
      "  ! \" ' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J L M N O P Q R S T U V X Y Z ` a b c d e f g h i j l m n o p q r s t u v x y z À Â Ç É Ê Ô à â ç è é ê ë î ï ô ù û ﻿\n",
      "[[  0.  32.]\n",
      " [ 59.   0.]]\n",
      "[[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = sorted(set(text))  # my vocabulary (many letters)\n",
    "print(\"I have\", len(vocab), \"different elements in  my text which are :\")\n",
    "print(' '.join(vocab))\n",
    "\n",
    "\n",
    "def sample_gen(batch_size, n_items):\n",
    "    \"\"\"Return a random sample\"\"\"\n",
    "    while True:\n",
    "        permutations = list(np.random.permutation(len(text) - n_items))\n",
    "        while len(permutations) > n_items + 1:\n",
    "            # Generate a batch\n",
    "            batch = []\n",
    "            for i in range(batch_size):\n",
    "                p = permutations.pop()\n",
    "                batch.append(text[p : p + n_items])\n",
    "            yield batch\n",
    "\n",
    "def encode_batch(batch, one_hot=False):\n",
    "    \"\"\"Takes a batch of string as input and encode it to a numerical\n",
    "    batch\"\"\"\n",
    "    batch_new = np.ndarray((len(batch),len(batch[0])))\n",
    "    batch_one = np.ndarray((len(batch), len(batch[0]), len(vocab))) #len batch = 2 : number of items in the array\n",
    "                                                                    #len vocab = 90 : number of different elements\n",
    "    \n",
    "    if one_hot == True:  #If true, then the number corresponding to the character will be a 1 on its value position , while \n",
    "                         #all other digits within the array will be 0\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[0])):\n",
    "                batch_one[i][j] = np.eye(1, len(vocab), vocab.index(batch[i][j]))\n",
    "        return batch_one\n",
    "    else:\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[0])):\n",
    "                batch_new[i][j] = vocab.index(batch[i][j])\n",
    "        return batch_new\n",
    "\n",
    "\n",
    "a = sample_gen(2, 2)\n",
    "b = next(a)\n",
    "\n",
    "print(encode_batch(b, one_hot=False))\n",
    "print(encode_batch(b, one_hot=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of training taken from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-85b048b9e3bf>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-85b048b9e3bf>\"\u001b[1;36m, line \u001b[1;32m55\u001b[0m\n\u001b[1;33m    with tf.name_scope('metrics')\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 100\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = len(vocab)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = len(vocab)\n",
    "\n",
    "# tf Graph input\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "W1 = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "B1 = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "def RNN(x, W1, B1):\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], W1) + B1\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    logits = RNN(X, W1, B1)\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "with tf.name_scope('metrics')\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-41bb0f4266a5>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-41bb0f4266a5>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    batch_x, batch_y = ??\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = ??\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model (6/10)\n",
    "Train a model that can learn to create text from a given input (letter wise)\n",
    "\n",
    "Dont forget to explain what you do, why, and if it do look to be working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file ./fables.txt\n",
      "Training text size is 0.47MB with 0.00KB set aside for validation. There will be 163 batches per epoch\n",
      "\n",
      "   0 (epoch 0) fables.txt │ \u0000\u0000\u0000\u0000\u0000 Monseigneur le Dauphin\\  │                                │ loss: 4.46961\n",
      "  30 (epoch 0) fables.txt │  j'aie seulement excit\u0000\u0000 les a │                                │ loss: 4.46495\n",
      "  60 (epoch 0) fables.txt │ que nous sommes l'abr\u0000\u0000g\u0000\u0000 de  │                                │ loss: 4.49148\n",
      "  90 (epoch 0) fables.txt │ \u0000\u0000-dire des deux personnages q │                                │ loss: 4.46177\n",
      " 120 (epoch 0) fables.txt │ endre, et le mena\u0000\u0000a que ses m │                                │ loss: 4.43034\n",
      " 150 (epoch 0) fables.txt │  son esprit, que les choses s' │                                │ loss: 4.45112\n",
      " 180 (epoch 0) fables.txt │ ne fit servir que le m\u0000\u0000me met │                                │ loss: 4.45336\n",
      " 210 (epoch 0) fables.txt │ i toutefois les dieux l'ordonn │                                │ loss: 4.48003\n",
      " 240 (epoch 0) fables.txt │ oir qu'il en avait comme magis │                                │ loss: 4.50186\n",
      " 270 (epoch 0) fables.txt │  Roi d'\u0000\u0000gypte, il n'en fit qu │                                │ loss: 4.46315\n",
      " 300 (epoch 0) fables.txt │ ait \u0000\u0000 votre demande \", reprit │                                │ loss: 4.44914\n",
      " 330 (epoch 0) fables.txt │ son bec un fromage.\\Ma\u0000\u0000tre Re │                                │ loss: 4.50361\n",
      " 360 (epoch 0) fables.txt │ choir encor :\\Ce droit, vous l │                                │ loss: 4.48734\n",
      " 390 (epoch 0) fables.txt │ le ;\\Son camarade le suit.\\ \\L │                                │ loss: 4.48581\n",
      " 420 (epoch 0) fables.txt │ province conquise :\\Un quart V │                                │ loss: 4.49657\n",
      " 450 (epoch 0) fables.txt │ r,\\Il met bas son fagot, il so │                                │ loss: 4.48121\n",
      " 480 (epoch 0) fables.txt │ ouches \u0000\u0000 miel\\ \\\u0000\u0000 l'oeuvre o │                                │ loss: 4.45769\n",
      " 510 (epoch 0) fables.txt │ mal d'\u0000\u0000crire en si haut style │                                │ loss: 4.47391\n",
      " 540 (epoch 0) fables.txt │ r\\Chez une autre Belette aux O │                                │ loss: 4.46395\n",
      " 570 (epoch 0) fables.txt │ e fasse peur ni me soucie ?\\   │                                │ loss: 4.39541\n",
      " 600 (epoch 0) fables.txt │ re b\u0000\u0000te,\\      Tandis qu'\u0000\u0000 p │                                │ loss: 4.42502\n",
      " 630 (epoch 0) fables.txt │  stratag\u0000\u0000me ;\\      Et notre  │                                │ loss: 4.39105\n",
      " 660 (epoch 0) fables.txt │ ur troupe n'\u0000\u0000tait pas encore  │                                │ loss: 4.46590\n",
      " 690 (epoch 0) fables.txt │ \u0000\u0000.\\Ces deux rivaux d'Horace,  │                                │ loss: 4.50866\n",
      " 720 (epoch 0) fables.txt │ frit ; les forces se perdirent │                                │ loss: 4.46584\n",
      " 750 (epoch 0) fables.txt │  je loue\\      Les gens bien s │                                │ loss: 4.34326\n",
      " 780 (epoch 0) fables.txt │ emeurer : voil\u0000\u0000 sa toile ourd │                                │ loss: 4.46484\n",
      " 810 (epoch 0) fables.txt │ m\u0000\u0000chants guerre continuelle.\\ │                                │ loss: 4.48188\n",
      " 840 (epoch 0) fables.txt │  Souris \u0000\u0000taient prisonni\u0000\u0000res │                                │ loss: 4.50484\n",
      " 870 (epoch 0) fables.txt │ ce,\\      Qu'un sou, quand il  │                                │ loss: 4.40535\n",
      " 900 (epoch 0) fables.txt │ re chou.\\On le qu\u0000\u0000te ; on le  │                                │ loss: 4.43794\n",
      " 930 (epoch 0) fables.txt │  tourne la t\u0000\u0000te,\\Et le Magot  │                                │ loss: 4.46816\n",
      " 960 (epoch 0) fables.txt │ uatri\u0000\u0000me Fable 11\\ \\ \\Tribut  │                                │ loss: 4.47670\n",
      " 990 (epoch 0) fables.txt │ et le Buste\\ \\Les Grands, pour │                                │ loss: 4.46118\n",
      "1020 (epoch 0) fables.txt │ ant : \" Je le donne aux plus f │                                │ loss: 4.42323\n",
      "1050 (epoch 0) fables.txt │ ra\u0000\u0000che et fourrage,\\      Com │                                │ loss: 4.45658\n",
      "1080 (epoch 0) fables.txt │ faire.\\Non qu'il faille bannir │                                │ loss: 4.50380\n",
      "1110 (epoch 0) fables.txt │ \\Pour faire un plat. Quel plat │                                │ loss: 4.48019\n",
      "1140 (epoch 0) fables.txt │ imaux quittent tous la maison\\ │                                │ loss: 4.47762\n",
      "1170 (epoch 0) fables.txt │ \\      Belle le\u0000\u0000on pour les g │                                │ loss: 4.38213\n",
      "1200 (epoch 0) fables.txt │ d n'en fit pas \u0000\u0000 demi.\\Ses re │                                │ loss: 4.46259\n",
      "1230 (epoch 0) fables.txt │ P\u0000\u0000tre, en sa fable.\\J'ai suiv │                                │ loss: 4.49457\n",
      "1260 (epoch 0) fables.txt │ emp\u0000\u0000rature des cieux.\\      S │                                │ loss: 4.42040\n",
      "1290 (epoch 0) fables.txt │ tal d'une fontaine\\      Un Ce │                                │ loss: 4.40348\n",
      "1320 (epoch 0) fables.txt │ ur d'hiver se promenant\\       │                                │ loss: 4.42204\n",
      "1350 (epoch 0) fables.txt │ de-moi, si ton dos\\      A por │                                │ loss: 4.40947\n",
      "1380 (epoch 0) fables.txt │ op verser de larmes :\\Qu'a bes │                                │ loss: 4.47745\n",
      "1410 (epoch 0) fables.txt │ r de vous plaire,\\Je croirai l │                                │ loss: 4.48359\n",
      "1440 (epoch 0) fables.txt │ cha\u0000\u0000n\u0000\u0000e ?\\      Et que pourr │                                │ loss: 4.38544\n",
      "1470 (epoch 0) fables.txt │ es soins ne purent faire\\Qu'el │                                │ loss: 4.47857\n",
      "1500 (epoch 0) fables.txt │ rs et les Pigeons\\ \\Mars autre │                                │ loss: 4.47566\n",
      "1530 (epoch 0) fables.txt │ doux :\\Une flatteuse erreur em │                                │ loss: 4.46889\n",
      "1560 (epoch 0) fables.txt │ t qu'au Japon\\La Fortune pour  │                                │ loss: 4.48612\n",
      "1590 (epoch 0) fables.txt │  fonder ce prologue\\Sur gens d │                                │ loss: 4.46027\n",
      "1620 (epoch 0) fables.txt │ \\      Sa soeur, et non sa sui │                                │ loss: 4.37677\n",
      "1650 (epoch 0) fables.txt │  monde entier accro\u0000\u0000tra sa ri │                                │ loss: 4.46885\n",
      "1680 (epoch 0) fables.txt │ \\Veut qu'on aille enfumer Rena │                                │ loss: 4.47426\n",
      "1710 (epoch 0) fables.txt │ it-il, tu devais bien purger\\L │                                │ loss: 4.49350\n",
      "1740 (epoch 0) fables.txt │ ient davantage.\\\" N'en puis-je │                                │ loss: 4.49063\n",
      "1770 (epoch 0) fables.txt │ tendu sur la place il le couch │                                │ loss: 4.44205\n",
      "1800 (epoch 0) fables.txt │  on se pla\u0000\u0000t\\      Toute seul │                                │ loss: 4.35814\n",
      "1830 (epoch 0) fables.txt │ our l'\u0000\u0000viter.\\ \\      Un P\u0000\u0000r │                                │ loss: 4.43398\n",
      "1860 (epoch 0) fables.txt │ perd\u0000\u0000t un coup de dent.\\      │                                │ loss: 4.36376\n",
      "1890 (epoch 0) fables.txt │ u que nos gens habitaient :\\   │                                │ loss: 4.43999\n",
      "1920 (epoch 0) fables.txt │ t Chat en use les matins.\\Ce r │                                │ loss: 4.46342\n",
      "1950 (epoch 0) fables.txt │ out : ce corps demeurera\\      │                                │ loss: 4.38813\n",
      "1980 (epoch 0) fables.txt │ ! dit-il, je te promets un tem │                                │ loss: 4.45939\n",
      "2010 (epoch 0) fables.txt │ \u0000\u0000re ?\\      L'absence est le  │                                │ loss: 4.39522\n",
      "2040 (epoch 0) fables.txt │ blables,\\      N'ont que l'hab │                                │ loss: 4.46071\n",
      "2070 (epoch 0) fables.txt │ in\\      Le traite en fr\u0000\u0000re.  │                                │ loss: 4.39911\n",
      "2100 (epoch 0) fables.txt │ \u0000\u0000 l'\u0000\u0000gard de la dent il fall │                                │ loss: 4.46290\n",
      "2130 (epoch 0) fables.txt │ s\u0000\u0000, l'on ne se souvient gu\u0000\u0000r │                                │ loss: 4.44867\n",
      "2160 (epoch 0) fables.txt │ i ne duit pas\\\u0000\u0000 gens peu curi │                                │ loss: 4.43517\n",
      "2190 (epoch 0) fables.txt │  devant qu'il f\u0000\u0000t nuit,\\      │                                │ loss: 4.41052\n",
      "2220 (epoch 0) fables.txt │ haque Castor agit : commune en │                                │ loss: 4.47186\n",
      "2250 (epoch 0) fables.txt │ de l'\u0000\u0000me\\Nous donner quelque  │                                │ loss: 4.45634\n",
      "2280 (epoch 0) fables.txt │ t \u0000\u0000 toute force avoir cause g │                                │ loss: 4.44349\n",
      "2310 (epoch 0) fables.txt │  cela, c'est un mal. Veux-tu l │                                │ loss: 4.46221\n",
      "2340 (epoch 0) fables.txt │ ns plut\u0000\u0000t ces gens :\\      Ju │                                │ loss: 4.38684\n",
      "2370 (epoch 0) fables.txt │ vous reprends : sortons de ces │                                │ loss: 4.44570\n",
      "2400 (epoch 0) fables.txt │  Lionne et l'Ourse\\ \\      M\u0000\u0000 │                                │ loss: 4.40500\n",
      "2430 (epoch 0) fables.txt │  grande\\S'\u0000\u0000vanouit bient\u0000\u0000t ; │                                │ loss: 4.50111\n",
      "2460 (epoch 0) fables.txt │ ui dit-il, Lionceau mon voisin │                                │ loss: 4.47541\n",
      "2490 (epoch 0) fables.txt │  au comble de la joie !\\Pourqu │                                │ loss: 4.45630\n",
      "2520 (epoch 0) fables.txt │ C'est beaucoup de pouvoir mod\u0000 │                                │ loss: 4.49627\n",
      "2550 (epoch 0) fables.txt │  l'apparence.\\Le conseil en es │                                │ loss: 4.46922\n",
      "2580 (epoch 0) fables.txt │ iens se joue \u0000\u0000galement.\\Nos t │                                │ loss: 4.48698\n",
      "2610 (epoch 0) fables.txt │ aire autant :\\Il ne tient pas  │                                │ loss: 4.45886\n",
      "2640 (epoch 0) fables.txt │ \u0000mes p\u0000\u0000nates ;\\Le Chat \u0000\u0000tait │                                │ loss: 4.48878\n",
      "2670 (epoch 0) fables.txt │  Toutes deux tomb\u0000\u0000rent dans l │                                │ loss: 4.47674\n",
      "2700 (epoch 0) fables.txt │ plus approcher\\      Pendant l │                                │ loss: 4.43013\n",
      "2730 (epoch 0) fables.txt │ ge,\\Et laisse seulement une br │                                │ loss: 4.45478\n",
      "2760 (epoch 0) fables.txt │     Voulut orner vos jeunes an │                                │ loss: 4.40872\n",
      "2790 (epoch 0) fables.txt │ n d'avidit\u0000\u0000.\\\" Je les vais de │                                │ loss: 4.46499\n",
      "2820 (epoch 0) fables.txt │  Chien, maudit instrument\\     │                                │ loss: 4.43634\n",
      "2850 (epoch 0) fables.txt │ ice : \" Accourez,\\      Un ani │                                │ loss: 4.43866\n",
      "2880 (epoch 0) fables.txt │ mort.\\ \\Livre douzi\u0000\u0000me Fable  │                                │ loss: 4.51277\n",
      "2910 (epoch 0) fables.txt │ ndit de semblables panneaux ;\\ │                                │ loss: 4.49637\n",
      "2940 (epoch 0) fables.txt │ Arbitre, l'Hospitalier et le S │                                │ loss: 4.50654\n",
      "2970 (epoch 0) fables.txt │ ente et sage,\\Consulte son Voi │                                │ loss: 4.47730\n",
      "└─────INDEX─────BOOK NAME─┴───────TRAINING SEQUENCE────────┴───────PREDICTED SEQUENCE───────┴────LOSS─────┘\n",
      "\n",
      "TRAINING STATS: batch 0/163 in epoch 0,      batch loss: 4.45388, batch accuracy: 0.20100\n",
      "\n",
      "┌───────────────────────────────────Generating random text from learned state───────────────────────────────────┐\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esb\u0000ruan\u0000ernesc\u0000s\u0000 sit ssu nttrn iue iiesnveeuqessursen siOsurtsntuuOsr csec sienecOst eirs rssi nensur seniOs qernOntnins{te ece neruotteeunroi nvrnvrrcrisnvitn tevvrenvune uesuib ouue e ouiuiu1u1rs  ns unsineiein triEEqvttnnnsuveurie iO sitOeqrqeequviuunssncuunsOeeuOnseersu irOeOsuean sOia raneeoernrenu iiru  uuisrn e enur iie eusqus uuei u -n tn s e  u1or rni  renooe-i eune  ouronoinnutsur{ u{-ntn n-r ssi-rv ss etuu iboerrin riiOsi itnn inve{vuev {iusuun{i nvuinieirein  nrtvu rnquutvOvstuburuiicisiis ve!see eenOrereusOie rOurrsssO-i  rn re OternsnOeOiuusotnirn i n-cqttcs {srrrt stsse  rrun icciisresevirqO  uO n i u   in u r truso eu seon-siis seesqq ur-torsuOurruci-cr-quOsOiee!c!ssu  u!n  i itiu tn  noni  rinevruuouo  s  ri in1un rsse - rs-   t   itstniretsouitsisti cu ir{ur qqrne n  rsiiOrtnveeeeOeseonieo{ -rv-nn euu t{nrt ii turcr  esie euuvts Oe  s nietes nunn e1uuiensn1u -re-s-rr n- eir OOuuOeuourOnen-resiu-- r  eOOe rr - ru e e  erti -ri  -s  oinooiut   nnsnoieeunnus ru u1r-t-n\n",
      "└───────────────────────────────────────────────End of generation───────────────────────────────────────────────┘\n",
      "Saved file: checkpoints/rnn_train_1513806876-0\n",
      "\n",
      "0%                                        Training on next 50 batches                                        100%\n",
      "===================================="
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-54f9e81e0854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;31m# train on one minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mistate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpkeep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdropout_pkeep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBATCHSIZE\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mostate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###We did this exercice within our hadoop project before we know there will be ann RNN python application.\n",
    "###Because it allows to make a text generation prediction based on some inputs we thought it was relevant to use it here.\n",
    "###Here is the source we based our code on : https://github.com/martin-gorner/tensorflow-rnn-shakespeare\n",
    "###Just so you know, we don't use an RNN but a GRU (Gated recurrent units) network.\n",
    "### comments with ### come from us\n",
    "### Because the computation is very slow, you can find screenshots of results in the folder \"Screenshots for results\"\n",
    "###Once lanched, you will not be able to launch again the cell if you interrupt it in the meantime \n",
    " \n",
    "# encoding: UTF-8\n",
    "# Copyright 2017 Google.com\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import rnn  # rnn stuff temporarily in contrib, moving back to code in TF 1.1\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import my_txtutils as txt\n",
    " \n",
    " \n",
    "tf.set_random_seed(1)\n",
    " \n",
    "# model parameters\n",
    "#\n",
    "# Usage:\n",
    "#   Training only:\n",
    "#         Leave all the parameters as they are\n",
    "#         Disable validation to run a bit faster (set validation=False below)\n",
    "#         You can follow progress in Tensorboard: tensorboard --log-dir=log\n",
    "#   Training and experimentation (default):\n",
    "#         Keep validation enabled\n",
    "#         You can now play with the parameters anf follow the effects in Tensorboard\n",
    "#         A good choice of parameters ensures that the testing and validation curves stay close\n",
    "#         To see the curves drift apart (\"overfitting\") try to use an insufficient amount of\n",
    "#         training data (shakedir = \"shakespeare/t*.txt\" for example)\n",
    "#\n",
    " \n",
    "SEQLEN = 30   ###Sequence of inputs (characters)\n",
    "BATCHSIZE = 100 #### Number of samples within our network. Here it means that it will be 100 times 30 char in each batch\n",
    "ALPHASIZE = txt.ALPHASIZE ### Supported alaphabet. In our case it's ASCII-7\n",
    "INTERNALSIZE = 512 ### Size of GRU cell . We can shoose it freely\n",
    "NLAYERS = 3 ###Number of layers within the network\n",
    "learning_rate = 0.001  ### fixed learning rate\n",
    "dropout_pkeep = 0.8    ### some dropout (he will drop 20% of randomly selected neurons will be ignored during training)\n",
    "#The activation of these neuron are removed. As a consequence any weight updates throught theses neuron is done in the backward process.\n",
    " \n",
    " \n",
    "# load data (fables.txt file)\n",
    "fabledir = \"./fables.txt\" #our input\n",
    "codetext, valitext, bookranges = txt.read_data_files(fabledir, validation=True)\n",
    " \n",
    " \n",
    "# display some stats on the data\n",
    " \n",
    "epoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\n",
    "txt.print_data_stats(len(codetext), len(valitext), epoch_size)\n",
    " \n",
    " \n",
    "#\n",
    "# the model (see FAQ in README.md)\n",
    "#\n",
    " \n",
    "lr = tf.placeholder(tf.float32, name='lr')  # learning rate\n",
    "pkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\n",
    "batchsize = tf.placeholder(tf.int32, name='batchsize')\n",
    " \n",
    "# inputs\n",
    " \n",
    "X = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ] #The text input for each batch\n",
    "Xo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ] #The text one_hot encoded where all char are have a 92 subdimension with a 1 in the letter they correspond\n",
    "### expected outputs = same sequence shifted by 1 since we are trying to predict the next character\n",
    "Y_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ] #The text output that should follow the sequence of input\n",
    "Yo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ] #The output one-hot encoded\n",
    " \n",
    "# input state\n",
    "Hin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n",
    " \n",
    "# using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\n",
    "# dynamic_rnn infers SEQLEN from the size of the inputs Xo\n",
    " \n",
    "# How to properly apply dropout in RNNs: see README.md\n",
    "cells = [rnn.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n",
    "###Generate 3 grucells of 512 units. \n",
    "###This command generated automatically the weights and biases of the gru cell, \n",
    "###that's why we don't need to create in our model. \n",
    " \n",
    " \n",
    "# \"naive dropout\" implementation\n",
    "dropcells = [rnn.DropoutWrapper(cell,input_keep_prob=pkeep) for cell in cells] \n",
    "###Here the neurons are dropped respectively to the probability specified to be keep (80%)\n",
    "multicell = rnn.MultiRNNCell(dropcells, state_is_tuple=False) \n",
    "###Now we get our 3 cells after the drop. They are now put in a multicell that stack our cells to generated a 3 deep model\n",
    "multicell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)  \n",
    "### dropout for the softmax layer same that for each cell, we remove 20% of the neuron that will be not activated temporaly\n",
    " \n",
    "Yr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin) #We enroll the cell to obtain a 3 deep cell of 30 characters (here our X will always have the same size)\n",
    "# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\n",
    "# H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\n",
    " \n",
    "H = tf.identity(H, name='H')  # just to give it a name\n",
    " \n",
    "# Softmax layer implementation:\n",
    "# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n",
    "# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n",
    " \n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ] #The output of the multicell computation that get 2D\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ] #The output become sequence of results for each possible letter\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ] #The real output get the same action to be at correct dimension\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]  #We get the loss comparing the output from the multicell and the real output\n",
    "loss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ] #The loss is reshaped to correspond to each sequence of characters\n",
    "Yo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ] #Softmax is apply to get a value between 0 and 1 \n",
    "Y = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ] #Argmax to get the higher value between the 92 possible char. This value will be 1 and the other one 0. \n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ] #Here the reshape will get the number which have 1 (the value of the preicted letter)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss) #AdamOptimizer is used \n",
    " \n",
    " \n",
    " \n",
    "# stats for display\n",
    "#You don't need to care about this session. it is for visualisation checkpoints and backup\n",
    " \n",
    "seqloss = tf.reduce_mean(loss, 1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n",
    "loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n",
    "acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n",
    "summaries = tf.summary.merge([loss_summary, acc_summary])\n",
    " \n",
    "# Init Tensorboard stuff. This will save Tensorboard information into a different\n",
    "# folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n",
    "# you can compare training and validation curves visually in Tensorboard.\n",
    " \n",
    "timestamp = str(math.trunc(time.time()))\n",
    "summary_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-training\")\n",
    "validation_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-validation\")\n",
    " \n",
    "# Init for saving models. They will be saved into a directory named 'checkpoints'.\n",
    "# Only the last checkpoint is kept.\n",
    " \n",
    "if not os.path.exists(\"checkpoints\"):\n",
    " \n",
    "    os.mkdir(\"checkpoints\")\n",
    "saver = tf.train.Saver(max_to_keep=1000)\n",
    " \n",
    " \n",
    " \n",
    "# for display: init the progress bar\n",
    " \n",
    "DISPLAY_FREQ = 50\n",
    "_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
    "progress = txt.Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n",
    " \n",
    "# init\n",
    "istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0\n",
    " \n",
    "# training loop\n",
    "#For all the input text for 12 iteration \n",
    "#x the input sequence, y_ the output that should be predicted\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=12):\n",
    "    \n",
    "\n",
    "    ###Training part###\n",
    "\n",
    "    # train on one minibatch\n",
    "    feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
    "    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
    "    \n",
    "    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n",
    "    if step % _50_BATCHES == 0:\n",
    " \n",
    "        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
    "        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n",
    "        summary_writer.add_summary(smm, step)\n",
    " \n",
    " \n",
    "    # run a validation step every 50 batches\n",
    "    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n",
    "    # so we cut it up and batch the pieces (slightly inaccurate)\n",
    "    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n",
    "    \n",
    "    ###Testing part###\n",
    "    if step % _50_BATCHES == 0 and len(valitext) > 0:\n",
    " \n",
    "        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n",
    "        bsize = len(valitext) // VALI_SEQLEN\n",
    "        txt.print_validation_header(len(codetext), bookranges)\n",
    "        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n",
    "        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
    "        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n",
    "                     batchsize: bsize}\n",
    " \n",
    "        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        txt.print_validation_stats(ls, acc)\n",
    " \n",
    "        # save validation data for Tensorboard\n",
    "        validation_writer.add_summary(smm, step)\n",
    " \n",
    " \n",
    " \n",
    "    # display a short text generated with the current weights and biases (every 150 batches)\n",
    "\n",
    "    ###It will be a 1000 char text based on the actuel weights and bias of our GRUs\n",
    " \n",
    "    if step // 3 % _50_BATCHES == 0:\n",
    "        txt.print_text_generation_header()\n",
    "        ry = np.array([[txt.convert_from_alphabet(ord(\"K\"))]])\n",
    "        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
    " \n",
    "        for k in range(1000):\n",
    " \n",
    "            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n",
    "            rc = txt.sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n",
    "            print(chr(txt.convert_to_alphabet(rc)), end=\"\")\n",
    "            ry = np.array([[rc]])\n",
    " \n",
    "        txt.print_text_generation_footer()\n",
    " \n",
    " \n",
    " \n",
    "    # save a checkpoint (every 500 batches)\n",
    " \n",
    "    if step // 10 % _50_BATCHES == 0:\n",
    " \n",
    "        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n",
    "        print(\"Saved file: \" + saved_file)\n",
    " \n",
    " \n",
    " \n",
    "    # display progress bar\n",
    "    progress.step(reset=step % _50_BATCHES == 0)\n",
    " \n",
    " \n",
    "    # loop state around\n",
    " \n",
    "    istate = ostate\n",
    "    step += BATCHSIZE * SEQLEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model (4/10)\n",
    "Train a model that can learn to create text from a given input (text wise). Using a word embeding seen in class, like CBOW\n",
    "\n",
    "Dont forget to explain what you do, why, and if it do look to be working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
